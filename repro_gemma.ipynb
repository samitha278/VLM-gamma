{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUWNsjyqm3u8fVakjwC8ju",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samitha278/VLM-gamma/blob/main/repro_gemma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nCDjy_TwWUfI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemma Config"
      ],
      "metadata": {
        "id": "Xusy_zE7_8oQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GemmaConfig():\n",
        "\n",
        "    def __init__(self,vocab_size,n_embd,n_hidden,n_head,n_kv_head,n_layer,head_dim=256,max_pos_embds=8192,rms_norm_eps=1e-6,rope_theta=1000.0,attention_bias=False,attention_dropot=0.0,pad_token_id=None):\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.n_embd = n_embd\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_head = n_head\n",
        "        self.n_kv_head = n_kv_head\n",
        "        self.n_layer = n_layer\n",
        "        self.head_dim = head_dim\n",
        "        self.max_pos_embds = max_pos_embds\n",
        "        self.rms_norm_eps = rms_norm_eps\n",
        "        self.rope_theta = rope_theta\n",
        "        self.attention_bias = attention_bias\n",
        "        self.attention_dropot = attention_dropot\n",
        "        self.pad_token_id = pad_token_id"
      ],
      "metadata": {
        "id": "-XMdNrpy__-G"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VLMGamma Cofig"
      ],
      "metadata": {
        "id": "dO1EoEQO4nOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VLMGammaConfig():\n",
        "\n",
        "    def __init__(self,ignore_index=-100,img_token_index=256000,vocab_size=257152,projection_dim=2048,n_embd=2048,pad_token_id=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.siglip_config = SigLipViTConfig()\n",
        "        self.gemma_config = GemmaConfig()\n",
        "\n",
        "        self.ignore_index = ignore_index\n",
        "        self.img_token_index = img_token_index\n",
        "        self.vocab_size - vocab_size\n",
        "        self.projection_dim = projection_dim\n",
        "        self.n_embd = n_embd\n",
        "        self.pad_token_id = pad_token_id\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N0qs1KXu5ljA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VLMGamma"
      ],
      "metadata": {
        "id": "1KYaDbJQ4lCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VLMGamma(nn.Module):\n",
        "\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        self.vision_encoder = SigLipViT(config.siglip_config)\n",
        "        self.projector = VLProjector(config)\n",
        "\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.gemma = GemmaCausalLM(config.gemma_config)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def tie_weights(self):\n",
        "        return self.transformer_decoder.tie_weights()\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,tokens,images,attention_mask,kv_cache):\n",
        "\n",
        "        input_embds = self.transformer_decoder.get_input_embds()(tokens)\n",
        "\n",
        "        vit_out_embds = self.vision_encoder(images)\n",
        "        proj_vit_out_embds = self.projector(vit_out_embds)\n",
        "\n",
        "        input_embds_merged = self.merge_text_img(tokens,proj_vit_out_embds,input_embds,attention_mask,kv_cache)\n",
        "\n",
        "\n",
        "        outputs = self.gemma(attention_mask,tokens,input_embds_merged,kv_cache)\n",
        ""
      ],
      "metadata": {
        "id": "yfJOvmE_XJjx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oe9BkCJV7aYb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}