{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjGs5nrHl0yg3DILu45CGD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samitha278/VLM-gamma/blob/main/repro_siglip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SigLip"
      ],
      "metadata": {
        "id": "2K6Az6UKpZbC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tdIPXO2tXmk4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Optional, Tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SigLip Config"
      ],
      "metadata": {
        "id": "Cc6Cgogk8zm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SigLipConfig:\n",
        "\n",
        "    def __init__(self,n_embd=768,n_hidden=3072,n_layer=12,n_head=12,n_channel=3,image_size=224,patch_size=16,ln_eps=1e-6,**kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        #model params\n",
        "        self.n_embd= n_embd\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_layer = n_layer\n",
        "        self.n_channel = n_channel\n",
        "\n",
        "        #data params\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_patch = (image_size // patch_size)**2\n",
        "\n",
        "        self.ln_eps = ln_eps\n"
      ],
      "metadata": {
        "id": "3tewxPVCpzFg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SigLip ViT Embeddings"
      ],
      "metadata": {
        "id": "V6VjNQ6y83TD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "\n",
        "    def __init__(self,config: SigLipConfig):\n",
        "        super.__init__()\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        self.patch_embds = nn.Conv2d(in_channels= config.n_channel, out_channels= config.n_embd, kernel_size=config.patch_size, stride= config.patch_size)\n",
        "\n",
        "        self.pos_embds = nn.Embedding(config.n_patch,embedding_dim=config.n_embd)\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        # x : [B,C,H,W]\n",
        "\n",
        "        patch_embds = self.patch_embds(x)    # [B, n_embd, n_patch**0.5, n_patch**0.5]\n",
        "        patch_embds.flatten(2)               # [B, n_embd, n_patch]\n",
        "        patch_embds.transpose(-1,-2)         # [B, n_patch, n_embd]\n",
        "\n",
        "        pos_embds = self.pos_embds(torch.arange(0,self.config.n_patch))\n",
        "\n",
        "        embds = patch_embds + pos_embds\n",
        "\n",
        "        return embds\n",
        "\n"
      ],
      "metadata": {
        "id": "WBAfEfHbx0xW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SigLip ViT Encoder Block"
      ],
      "metadata": {
        "id": "PDYubo6CosdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self,config:SigLipConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = Attention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        # x : [B,n_patch,n_embd]\n",
        "\n",
        "        out = x + self.attn(self.ln_1(x))\n",
        "        out = out + self.mlp(self.ln_2(x))\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "jd_-Qhj-oxle"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SigLip ViT"
      ],
      "metadata": {
        "id": "UAi9af6xJSYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SigLipViT(nn.Module):\n",
        "\n",
        "    def __init__(self, config: SigLipConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        self.embeddings = Embeddings(config)\n",
        "        self.encoder = nn.ModuleList([Block(config) for _ in config.n_layer])\n",
        "        self.ln = nn.LayerNorm(config.n_embd,eps=config.ln_eps)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        # x : [B,C,H,W]\n",
        "\n",
        "        embds = self.embeddings(x)\n",
        "\n",
        "        out = embds\n",
        "        for block in self.encoder:\n",
        "            out = block(out)\n",
        "\n",
        "        out = self.ln(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "9qqv2lDg9Ixw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3oXrTmAkq9r-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}