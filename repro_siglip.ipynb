{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCWrXk/DaBhmiksbn5TUtE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samitha278/VLM-gamma/blob/main/repro_siglip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SigLip"
      ],
      "metadata": {
        "id": "2K6Az6UKpZbC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tdIPXO2tXmk4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Optional, Tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SigLip Config"
      ],
      "metadata": {
        "id": "Cc6Cgogk8zm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SigLipConfig:\n",
        "\n",
        "    def __init__(self,n_embd=768,n_hidden=3072,n_layer=12,n_head=12,n_channel=3,image_size=224,patch_size=16,ln_eps=1e-6,**kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        #model params\n",
        "        self.n_embd= n_embd\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_layer = n_layer\n",
        "        self.n_head = n_head\n",
        "        self.n_channel = n_channel\n",
        "\n",
        "        #data params\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_patch = (image_size // patch_size)**2\n",
        "\n",
        "        self.ln_eps = ln_eps\n"
      ],
      "metadata": {
        "id": "3tewxPVCpzFg"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SigLip ViT Embeddings"
      ],
      "metadata": {
        "id": "V6VjNQ6y83TD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "\n",
        "    def __init__(self,config: SigLipConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        self.patch_embds = nn.Conv2d(in_channels= config.n_channel, out_channels= config.n_embd, kernel_size=config.patch_size, stride= config.patch_size)\n",
        "\n",
        "        self.pos_embds = nn.Embedding(config.n_patch,embedding_dim=config.n_embd)\n",
        "        self.register_buffer('pos_ids',torch.arange(0,config.n_patch, persistent=False)) # persistent =False ; don't save in checkpoint\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        # x : [B,C,H,W]\n",
        "\n",
        "        patch_embds = self.patch_embds(x)    # [B, n_embd, n_patch**0.5, n_patch**0.5]\n",
        "        patch_embds.flatten(2)               # [B, n_embd, n_patch]\n",
        "        patch_embds.transpose(-1,-2)         # [B, n_patch, n_embd]\n",
        "\n",
        "        pos_embds = self.pos_embds(self.pos_ids)\n",
        "\n",
        "        embds = patch_embds + pos_embds\n",
        "\n",
        "        return embds\n",
        "\n"
      ],
      "metadata": {
        "id": "WBAfEfHbx0xW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SigLip ViT Attention"
      ],
      "metadata": {
        "id": "5If8SG4GrdMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, config : SigLipConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        self.W = nn.Linear(config.n_embd,config.n_embd*3)\n",
        "\n",
        "        self.proj = nn.Linear(config.n_embd,config.n_embd)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        # x : [B,n_patch,n_embd]\n",
        "\n",
        "        qkv = self.W(x)    # [B,n_patch,n_embd*3]\n",
        "\n",
        "        print(qkv.shape)\n",
        "\n",
        "        query,key,value = torch.chunk(qkv,3,-1)  # each : [B,n_patch,n_embd]\n",
        "\n",
        "        print(query.shape)\n",
        "\n",
        "        weights = query @ key.transpose(-1,-2)          # [B,n_patch,n_patch]\n",
        "        weights = weights / (self.config.n_embd//self.config.n_head)**0.5\n",
        "\n",
        "        print(weights.shape)\n",
        "        out = weights @ value        # [B,n_patch,n_embd]\n",
        "\n",
        "        print(out.shape)\n",
        "\n",
        "        out = self.proj(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "wXNqqCCxrg5P"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn = Attention(SigLipConfig)\n",
        "attn(torch.randn(4,196,768))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NicWX3geL_gq",
        "outputId": "093db186-f1a5-4e5c-d95f-b318d5363b9b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 196, 2304])\n",
            "torch.Size([4, 196, 768])\n",
            "torch.Size([4, 196, 196])\n",
            "torch.Size([4, 196, 768])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[  3.2310,   5.0710,   9.6796,  ...,  -1.9782, -10.1048,  -5.9263],\n",
              "         [ -3.9312,   2.2673,   6.8564,  ...,   0.7830,  -2.3773, -10.9537],\n",
              "         [ -4.5310,   4.0279,  -7.3734,  ...,   1.6300,   3.4329,  -1.9785],\n",
              "         ...,\n",
              "         [ -6.9203,   1.8917,  -7.3212,  ...,  -6.4091,   5.6249,  -4.9347],\n",
              "         [  5.9090,   5.7038,  14.3785,  ...,  -9.4454,  -2.1515,  -5.6580],\n",
              "         [  1.4164,   3.7150,  -5.0036,  ...,   6.6115,   1.0287,   5.8579]],\n",
              "\n",
              "        [[ -0.9498,   3.0343,   4.6050,  ...,  -5.0981,  -5.2234,   0.8759],\n",
              "         [ -3.0259,   3.2398,   6.1334,  ...,  -5.7936,   2.8396,  -3.7130],\n",
              "         [ 11.8142,  -8.2874,   1.0808,  ...,  -3.5087,  -0.4313,  -2.9563],\n",
              "         ...,\n",
              "         [  9.2087,  -9.4692,  -4.8015,  ...,  -4.0036,   7.7564,  -5.1409],\n",
              "         [ -2.2968,   4.0281,  -5.6268,  ...,  -4.6646,   3.4678,  -4.9857],\n",
              "         [  7.6221,  -9.5990,  -1.7200,  ...,   6.4135,   0.0152,   0.3916]],\n",
              "\n",
              "        [[ -5.3154, -12.2872,   2.5653,  ...,  -5.0048,   9.2983,  -5.2799],\n",
              "         [  3.9227,  -7.0062,  -2.4759,  ...,  -5.0844,  -4.1315,  -6.6595],\n",
              "         [  7.1589,  13.1982,  -9.2425,  ...,  -9.4387,  -1.6167,  -3.0435],\n",
              "         ...,\n",
              "         [ -2.2028,  -0.2201,   1.5013,  ...,  -3.6464,  -3.2388,   0.0695],\n",
              "         [  0.0375,   2.5825,   2.0222,  ...,  -0.6787, -11.7930,  -0.0559],\n",
              "         [  3.5237,   0.7465,  -6.1401,  ...,   3.2784,   6.2085,   9.7201]],\n",
              "\n",
              "        [[  3.4330,   0.6109,  -1.8884,  ...,   7.2982,   5.3229,  -1.0114],\n",
              "         [ -4.5547,   1.0707,   2.2182,  ...,  -2.4009,   7.6385,  -5.7342],\n",
              "         [ -7.6026, -12.8715,  -1.1634,  ...,  -5.0379,  -8.4720,  -1.0849],\n",
              "         ...,\n",
              "         [ -1.7420,   2.1223,  -4.2269,  ...,   0.2935,  -2.8315,   6.4107],\n",
              "         [  5.0613,   2.8001,   0.6392,  ...,  -0.4072,  -6.9190,  -0.9989],\n",
              "         [ -6.0526,   4.1649,  -5.2823,  ...,   2.4014,  -0.4936,  -0.2272]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SigLip ViT Encoder Block"
      ],
      "metadata": {
        "id": "PDYubo6CosdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self,config:SigLipConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = Attention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        # x : [B,n_patch,n_embd]\n",
        "\n",
        "        out = x + self.attn(self.ln_1(x))\n",
        "        out = out + self.mlp(self.ln_2(x))\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "jd_-Qhj-oxle"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SigLip ViT"
      ],
      "metadata": {
        "id": "UAi9af6xJSYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SigLipViT(nn.Module):\n",
        "\n",
        "    def __init__(self, config: SigLipConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        self.embeddings = Embeddings(config)\n",
        "        self.encoder = nn.ModuleList([Block(config) for _ in config.n_layer])\n",
        "        self.ln = nn.LayerNorm(config.n_embd,eps=config.ln_eps)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        # x : [B,C,H,W]\n",
        "\n",
        "        embds = self.embeddings(x)\n",
        "\n",
        "        out = embds\n",
        "        for block in self.encoder:\n",
        "            out = block(out)\n",
        "\n",
        "        out = self.ln(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "9qqv2lDg9Ixw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3oXrTmAkq9r-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}