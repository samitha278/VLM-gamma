{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRzEdQGKBiZyv+OykdxOAp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samitha278/VLM-gamma/blob/main/repro_siglip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SigLip"
      ],
      "metadata": {
        "id": "2K6Az6UKpZbC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tdIPXO2tXmk4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Optional, Tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SigLip Config"
      ],
      "metadata": {
        "id": "Cc6Cgogk8zm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SigLipConfig:\n",
        "\n",
        "    def __init__(self,n_embd=768,n_hidden=3072,n_layer=12,n_head=12,n_channel=3,image_size=224,patch_size=16,ln_eps=1e-6,**kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        #model params\n",
        "        self.n_embd= n_embd\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_layer = n_layer\n",
        "        self.n_head = n_head\n",
        "        self.n_channel = n_channel\n",
        "\n",
        "        #data params\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_patch = (image_size // patch_size)**2\n",
        "\n",
        "        self.ln_eps = ln_eps\n"
      ],
      "metadata": {
        "id": "3tewxPVCpzFg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SigLip ViT Embeddings"
      ],
      "metadata": {
        "id": "V6VjNQ6y83TD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "\n",
        "    def __init__(self,config: SigLipConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        self.patch_embds = nn.Conv2d(in_channels= config.n_channel, out_channels= config.n_embd, kernel_size=config.patch_size, stride= config.patch_size)\n",
        "\n",
        "        self.pos_embds = nn.Embedding(config.n_patch,embedding_dim=config.n_embd)\n",
        "        self.register_buffer('pos_ids',torch.arange(0,config.n_patch, persistent=False)) # persistent =False ; don't save in checkpoint\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        # x : [B,C,H,W]\n",
        "\n",
        "        patch_embds = self.patch_embds(x)    # [B, n_embd, n_patch**0.5, n_patch**0.5]\n",
        "        patch_embds.flatten(2)               # [B, n_embd, n_patch]\n",
        "        patch_embds.transpose(-1,-2)         # [B, n_patch, n_embd]\n",
        "\n",
        "        pos_embds = self.pos_embds(self.pos_ids)\n",
        "\n",
        "        embds = patch_embds + pos_embds\n",
        "\n",
        "        return embds\n",
        "\n"
      ],
      "metadata": {
        "id": "WBAfEfHbx0xW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SigLip ViT Attention"
      ],
      "metadata": {
        "id": "5If8SG4GrdMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, config : SigLipConfig ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "        self.head_size = config.n_embd // config.n_head\n",
        "\n",
        "        self.W = nn.Linear(config.n_embd,config.n_embd*3)\n",
        "\n",
        "        self.proj = nn.Linear(config.n_Embd,config.n_embd)\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        B,n_patch,n_embd = x.shape\n",
        "\n",
        "        qkv = self.W(x)   # [B,n_patch,n_embd*3]\n",
        "\n",
        "        query, key, value = torch.chunk(qkv,3,-1)  # each : [B,n_patch,n_embd]\n",
        "\n",
        "        query = query.view(B,n_patch,self.config.n_head,self.head_size).transpose(1,2)\n",
        "        key   =   key.view(B,n_patch,self.config.n_head,self.head_size).transpose(1,2)\n",
        "        value = value.view(B,n_patch,self.config.n_head,self.head_size).transpose(1,2) # B,n_head,n_patch,head_size\n",
        "\n",
        "        weights = (query @ key.transpose(-1,-2))/(self.head_size**0.5)  # B,n_head,n_patch,n_patch\n",
        "\n",
        "        out = weights @ value         # B,n_head,n_patch,head_size\n",
        "\n",
        "        out = out.transpose(1,2).contiguous().view(B,self.config.n_patch,self.config.n_embd)\n",
        "        out = self.proj(out)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "QrXYje41Y0GW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class wrongAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config : SigLipConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        self.W = nn.Linear(config.n_embd,config.n_embd*3)\n",
        "\n",
        "        self.proj = nn.Linear(config.n_embd,config.n_embd)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        # x : [B,n_patch,n_embd]\n",
        "\n",
        "        qkv = self.W(x)    # [B,n_patch,n_embd*3]\n",
        "\n",
        "        query,key,value = torch.chunk(qkv,3,-1)  # each : [B,n_patch,n_embd]\n",
        "\n",
        "        weights = query @ key.transpose(-1,-2)          # [B,n_patch,n_patch]\n",
        "        weights = weights / (self.config.n_embd//self.config.n_head)**0.5\n",
        "\n",
        "        out = weights @ value        # [B,n_patch,n_embd]\n",
        "\n",
        "        out = self.proj(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "wXNqqCCxrg5P"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SigLip ViT MLP"
      ],
      "metadata": {
        "id": "pnrA8RGZcvDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self,config:SigLipConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        self.layer =  nn.Linear(config.n_embd,config.n_hidden)\n",
        "        self.gelu = nn.GELU\n",
        "        self.proj = nn.Linear(config.n_hidden,config.n_embd)\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        out = self.layer(x)\n",
        "        out = self.gelu(out)\n",
        "        out = self.proj(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "4JGJDML2cy41"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SigLip ViT Encoder Block"
      ],
      "metadata": {
        "id": "PDYubo6CosdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self,config:SigLipConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = Attention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        # x : [B,n_patch,n_embd]\n",
        "\n",
        "        out = x + self.attn(self.ln_1(x))\n",
        "        out = out + self.mlp(self.ln_2(x))\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "jd_-Qhj-oxle"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SigLip ViT"
      ],
      "metadata": {
        "id": "UAi9af6xJSYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SigLipViT(nn.Module):\n",
        "\n",
        "    def __init__(self, config: SigLipConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        self.embeddings = Embeddings(config)\n",
        "        self.encoder = nn.ModuleList([Block(config) for _ in config.n_layer])\n",
        "        self.ln = nn.LayerNorm(config.n_embd,eps=config.ln_eps)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        # x : [B,C,H,W]\n",
        "\n",
        "        embds = self.embeddings(x)\n",
        "\n",
        "        out = embds\n",
        "        for block in self.encoder:\n",
        "            out = block(out)\n",
        "\n",
        "        out = self.ln(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "9qqv2lDg9Ixw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3oXrTmAkq9r-"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}